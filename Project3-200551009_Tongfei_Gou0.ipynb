{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb8f2f4-083f-4499-996d-0543bf377b35",
   "metadata": {},
   "source": [
    "<!-- Project 3 – IMDB MOVIE REVIEW\n",
    "Due on Sat 9th Apr 11:59pm EST\n",
    "Context: IMDB dataset having 25K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous\n",
    "benchmark datasets. We provide a set of 12,500 highly polar movie reviews for training and 12,500 for\n",
    "testing. Please use less data eg 6K reviews if you are facing memory issues but make sure to use equal\n",
    "number of positive and negative sentiment reviews. Mention clearly in the notebook, if you have used a\n",
    "reduced dataset.\n",
    "For more dataset information, please go through the following link,\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "Dataset Source: Click here\n",
    "Task: Goal of this project is to predict the number of positive and negative reviews using classification\n",
    "Implementation:\n",
    "- Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and\n",
    "Lemmatize/Stem)\n",
    "- Perform TFIDF Vectorization\n",
    "- Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting\n",
    "Classifier. Use Xgboost instead of Gradient Boosting if it's taking a very long time in\n",
    "GridSearchCV\n",
    "- Perform Final evaluation of models on the best parameter settings using the evaluation metrics\n",
    "- Report the best performing model\n",
    "Submission Instructions: Please just submit one jupyter notebook containing all the code and make use\n",
    "of markdown cells to include the comments, answers, reasoning, analysis, etc.\n",
    "Note: Name of your file should be your “Project3-id_Firstname_Lastname.ipynb” -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c393b-9148-4be6-bec4-f85ecf7ac52f",
   "metadata": {},
   "source": [
    "### Project 3 – IMDB MOVIE REVIEW\n",
    "Due on Sat 9th Apr 11:59pm EST\n",
    "Context: IMDB dataset having 25K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous\n",
    "benchmark datasets. We provide a set of 12,500 highly polar movie reviews for training and 12,500 for\n",
    "testing. Please use less data eg 6K reviews if you are facing memory issues but make sure to use equal\n",
    "number of positive and negative sentiment reviews. Mention clearly in the notebook, if you have used a\n",
    "reduced dataset.\n",
    "#### For more dataset information, please go through the following link,\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "#### Dataset Source: Click here\n",
    "#### Task: Goal of this project is to predict the number of positive and negative reviews using classification\n",
    "Implementation:\n",
    "- Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and\n",
    "Lemmatize/Stem)\n",
    "- Perform TFIDF Vectorization\n",
    "- Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting\n",
    "Classifier. Use Xgboost instead of Gradient Boosting if it's taking a very long time in\n",
    "GridSearchCV\n",
    "- Perform Final evaluation of models on the best parameter settings using the evaluation metrics\n",
    "- Report the best performing model\n",
    "#### Submission Instructions: Please just submit one jupyter notebook containing all the code and make use of markdown cells to include the comments, answers, reasoning, analysis, etc.\n",
    "#### Note: Name of your file should be your “Project3-id_Firstname_Lastname.ipynb”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e279fcf6-6adb-49ac-86d3-e63a82073672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57a1013-bcfc-48e8-ac3d-8c69ee301175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I thought this was a wonderful way to spend ti...  positive\n",
       "1  Probably my all-time favorite movie, a story o...  positive\n",
       "2  I sure would like to see a resurrection of a u...  positive\n",
       "3  This show was an amazing, fresh & innovative i...  negative\n",
       "4  Encouraged by the positive comments about this...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "# Replace 'path_to_dataset' with the actual path to your dataset file\n",
    "imdb = pd.read_excel('IMDB_dataset.xlsx')\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e4b8e-010b-4cd6-a258-4f23055d826e",
   "metadata": {},
   "source": [
    "### Reduce the Dataset to 6000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ccb700-01cc-46d3-8e84-e66424e428e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset shape: (6000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly select an equal number of positive and negative sentiment comments from the dataset\n",
    "positive_reviews = imdb[imdb['sentiment'] == 'positive'].sample(n=3000, random_state=42)\n",
    "negative_reviews = imdb[imdb['sentiment'] == 'negative'].sample(n=3000, random_state=42)\n",
    "\n",
    "# Combine extracted positive and negative sentiment reviews into a smaller dataset\n",
    "imdb_sample = pd.concat([positive_reviews, negative_reviews])\n",
    "\n",
    "# Reset index\n",
    "imdb_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the shape of a smaller data set\n",
    "print(\"Reduced dataset shape:\", imdb_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437558e5-6b91-4143-84d1-5ac6e3cac8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Of course the average \"Sci-Fi\" Battle Star Gal...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sorry to say I have no idea what Hollywood is ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"The Lady from Shanghai\" is well known as one ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ed Harris and Cuba Gooding Jr. where cast perf...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kate Miller (Angie Dickinson) is having proble...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>Let me start out by saying that I used to real...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>What we have here is a classic case of TOO muc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>Oh it really really is. I've seen films that I...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>OK well i found this movie in my dads old pile...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>Yes, dumb is the word for this actress. I know...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     Of course the average \"Sci-Fi\" Battle Star Gal...  positive\n",
       "1     Sorry to say I have no idea what Hollywood is ...  positive\n",
       "2     \"The Lady from Shanghai\" is well known as one ...  positive\n",
       "3     Ed Harris and Cuba Gooding Jr. where cast perf...  positive\n",
       "4     Kate Miller (Angie Dickinson) is having proble...  positive\n",
       "...                                                 ...       ...\n",
       "5995  Let me start out by saying that I used to real...  negative\n",
       "5996  What we have here is a classic case of TOO muc...  negative\n",
       "5997  Oh it really really is. I've seen films that I...  negative\n",
       "5998  OK well i found this movie in my dads old pile...  negative\n",
       "5999  Yes, dumb is the word for this actress. I know...  negative\n",
       "\n",
       "[6000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =imdb_sample\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524bdfe-af1c-4254-a92e-f38e6ae0539a",
   "metadata": {},
   "source": [
    "### Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and Lemmatize/Stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9514307c-26b2-49f3-8258-18de1ed8caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Of course the average \"Sci-Fi\" Battle Star Gal...</td>\n",
       "      <td>positive</td>\n",
       "      <td>744</td>\n",
       "      <td>5.8</td>\n",
       "      <td>[course, average, scifi, battle, star, gallact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sorry to say I have no idea what Hollywood is ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>561</td>\n",
       "      <td>2.7</td>\n",
       "      <td>[sorry, say, idea, hollywood, sure, give, us, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"The Lady from Shanghai\" is well known as one ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[lady, shanghai, well, known, one, hollywoods,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ed Harris and Cuba Gooding Jr. where cast perf...</td>\n",
       "      <td>positive</td>\n",
       "      <td>617</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[ed, harris, cuba, gooding, jr, cast, perfectl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kate Miller (Angie Dickinson) is having proble...</td>\n",
       "      <td>positive</td>\n",
       "      <td>3643</td>\n",
       "      <td>4.3</td>\n",
       "      <td>[kate, miller, angie, dickinson, problems, mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  body_len  \\\n",
       "0  Of course the average \"Sci-Fi\" Battle Star Gal...  positive       744   \n",
       "1  Sorry to say I have no idea what Hollywood is ...  positive       561   \n",
       "2  \"The Lady from Shanghai\" is well known as one ...  positive      1350   \n",
       "3  Ed Harris and Cuba Gooding Jr. where cast perf...  positive       617   \n",
       "4  Kate Miller (Angie Dickinson) is having proble...  positive      3643   \n",
       "\n",
       "   punct%                                       clean_review  \n",
       "0     5.8  [course, average, scifi, battle, star, gallact...  \n",
       "1     2.7  [sorry, say, idea, hollywood, sure, give, us, ...  \n",
       "2     3.8  [lady, shanghai, well, known, one, hollywoods,...  \n",
       "3     4.1  [ed, harris, cuba, gooding, jr, cast, perfectl...  \n",
       "4     4.3  [kate, miller, angie, dickinson, problems, mar...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "df['body_len'] = df['review'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "df['punct%'] = df['review'].apply(lambda x: count_punct(x))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "df['clean_review'] = df['review'].apply(lambda x: clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0167d92-ddf5-42e2-886f-f1fc650667a3",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd8b507-9802-45c6-8f55-2368c74d925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['review', 'body_len', 'punct%']], df['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555b4b9-250f-461f-88b5-c64b6530407a",
   "metadata": {},
   "source": [
    "### Perform TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f135f5c8-5eab-4bc2-ac5d-a779a11529ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>50780</th>\n",
       "      <th>50781</th>\n",
       "      <th>50782</th>\n",
       "      <th>50783</th>\n",
       "      <th>50784</th>\n",
       "      <th>50785</th>\n",
       "      <th>50786</th>\n",
       "      <th>50787</th>\n",
       "      <th>50788</th>\n",
       "      <th>50789</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2639</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>562</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1916</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1760</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>468</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>740</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>705</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>1270</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>1053</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 50792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      body_len  punct%    0    1    2    3    4    5    6    7  ...  50780  \\\n",
       "0          510     4.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1         2639     5.3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "2          562     2.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "3         1916     4.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4         1760     4.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "...        ...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...   \n",
       "4795       468     6.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4796       740     3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4797       705     4.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4798      1270     5.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4799      1053     2.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "      50781  50782  50783  50784  50785  50786  50787  50788  50789  \n",
       "0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "4795    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4796    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4797    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4798    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4799    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[4800 rows x 50792 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['review'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['review'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['review'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2f9a5-75c8-4399-97ce-c322fb5a0fdf",
   "metadata": {},
   "source": [
    "### Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting Classifier. Use Xgboost instead of Gradient Boosting if it's taking a very long time in GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31e2af4-4974-4c33-98f3-35c01f850ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e023c6e-94e7-4af1-87d9-98b7fe2dc421",
   "metadata": {},
   "source": [
    "#### Exploring parameter settings using GridSearchCV on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636eef48-6f22-4fec-978e-d0bb62520cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 6.84 / Predict time: 0.387 ---- Precision: 0.879 / Recall: 0.811 / Accuracy: 0.841\n"
     ]
    }
   ],
   "source": [
    "# Convert feature name to string type\n",
    "X_train_vect.columns = X_train_vect.columns.astype(str)\n",
    "X_test_vect.columns = X_test_vect.columns.astype(str)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34976823-2984-4fee-9dbd-f0140ff505bf",
   "metadata": {},
   "source": [
    "#### Exploring parameter settings using GridSearchCV on Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1551f1dc-ed6d-49f8-84d6-5dcb2fa174c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 634.449 / Predict time: 0.553 ---- Precision: 0.836 / Recall: 0.843 / Accuracy: 0.829\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b3a76-e667-47b5-8a92-306f796849db",
   "metadata": {},
   "source": [
    "### Perform Final evaluation of models on the best parameter settings using the evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed624c48-6857-4ca7-99c0-8ac97e35566e",
   "metadata": {},
   "source": [
    "#### These two results evaluate the performance of a classification model, including training time, prediction time, precision, recall, and accuracy.\n",
    "\n",
    "#### Fit time: \n",
    "The training time for the first model is 6.84 seconds, while for the second model, it is 634.449 seconds. It's evident that the first model's training time is much shorter than the second model's training time.\n",
    "\n",
    "#### Predict time: \n",
    "The prediction time for the first model is 0.387 seconds, and for the second model, it is 0.553 seconds. The prediction time for both models is similar, but the second model is slightly slower.\n",
    "\n",
    "#### Precision: \n",
    "The precision of the first model is 0.879, whereas for the second model, it's 0.836. Precision measures the proportion of true positive predictions among all positive predictions. Hence, the first model has slightly higher precision than the second model.\n",
    "\n",
    "#### Recall:\n",
    "The recall of the first model is 0.811, and for the second model, it's 0.843. Recall assesses the model's ability to correctly identify all positive samples. Therefore, the second model has slightly higher recall than the first model.\n",
    "\n",
    "#### Accuracy:\n",
    "The accuracy of the first model is 0.841, and for the second model, it's 0.829. Accuracy measures the proportion of correctly predicted samples among all samples. Both models have similar accuracy, but the first model is slightly higher.\n",
    "\n",
    "#### In conclusion, although the first model has a shorter training time, it slightly lags behind the second model in terms of precision and recall. Therefore, the choice between these two models should be based on specific application scenarios and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e852877-52c7-4bbb-b6bb-323a8efd5d82",
   "metadata": {},
   "source": [
    "### Report the best performing model\n",
    "#### Taking various indicators into consideration, the first model is slightly better than the second model in precision and accuracy, but the second model is slightly better than the first model in recall rate. Therefore, considering precision, recall, and precision, the second model can be considered a better performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e24a1-1210-477b-af9e-033c9da67d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
